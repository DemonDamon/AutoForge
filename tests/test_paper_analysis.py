"""
ç®€å•çš„è®ºæ–‡åˆ†æåŠŸèƒ½æµ‹è¯•
"""

import os
import sys
import logging
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from autoforge.crawler import PapersWithCodeCrawler
from autoforge.crawler.paper_downloader import PaperDownloader
from autoforge.analyzers.paper_analyzer import PaperAnalyzer
from autoforge.llm import BaiLianClient

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

logger = logging.getLogger(__name__)


def test_basic_functionality():
    """æµ‹è¯•åŸºæœ¬åŠŸèƒ½"""
    
    logger.info("å¼€å§‹æµ‹è¯•è®ºæ–‡åˆ†æåŸºæœ¬åŠŸèƒ½...")
    
    # 1. æµ‹è¯•LLMå®¢æˆ·ç«¯åˆå§‹åŒ–
    api_key = os.environ.get("BAILIAN_API_KEY")
    model = os.environ.get("BAILIAN_MODEL", "qwen-plus")
    
    if not api_key:
        logger.warning("æœªè®¾ç½®BAILIAN_API_KEYç¯å¢ƒå˜é‡ï¼Œå°†è·³è¿‡LLMç›¸å…³æµ‹è¯•")
        llm_client = None
    else:
        try:
            llm_client = BaiLianClient(api_key=api_key, model=model)
            logger.info("âœ“ LLMå®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            logger.error(f"âœ— LLMå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}")
            return False
    
    # 2. æµ‹è¯•è¾“å‡ºç›®å½•åˆ›å»º
    try:
        output_dir = Path("outputs/test_paper_analysis")
        output_dir.mkdir(parents=True, exist_ok=True)
        logger.info("âœ“ è¾“å‡ºç›®å½•åˆ›å»ºæˆåŠŸ")
    except Exception as e:
        logger.error(f"âœ— è¾“å‡ºç›®å½•åˆ›å»ºå¤±è´¥: {e}")
        return False
    
    # 3. æµ‹è¯•ç»„ä»¶åˆå§‹åŒ–
    try:
        pwc_crawler = PapersWithCodeCrawler(output_dir=str(output_dir / "pwc_results"))
        logger.info("âœ“ PapersWithCodeCrawler åˆå§‹åŒ–æˆåŠŸ")
        
        paper_downloader = PaperDownloader(output_dir=str(output_dir / "papers"))
        logger.info("âœ“ PaperDownloader åˆå§‹åŒ–æˆåŠŸ")
        
        paper_analyzer = PaperAnalyzer(llm_client=llm_client, output_dir=str(output_dir))
        logger.info("âœ“ PaperAnalyzer åˆå§‹åŒ–æˆåŠŸ")
        
    except Exception as e:
        logger.error(f"âœ— ç»„ä»¶åˆå§‹åŒ–å¤±è´¥: {e}")
        return False
    
    # 4. æµ‹è¯•è®ºæ–‡æœç´¢ï¼ˆä½¿ç”¨å¤šä¸ªä¸åŒçš„æœç´¢è¯ï¼‰
    search_terms = [
        "attention mechanism",  # æ›´å…·ä½“çš„æœ¯è¯­
        "bert model",          # å…·ä½“çš„æ¨¡å‹å
        "neural network",      # é€šç”¨æœ¯è¯­
        "deep learning"        # é€šç”¨æœ¯è¯­
    ]
    
    search_success = False
    for search_term in search_terms:
        try:
            logger.info(f"æµ‹è¯•æœç´¢è¯: '{search_term}'")
            papers = pwc_crawler.search_papers(search_term, top_k=3)
            if papers:
                logger.info(f"âœ“ æœç´¢ '{search_term}' æˆåŠŸï¼Œæ‰¾åˆ° {len(papers)} ç¯‡è®ºæ–‡")
                for i, paper in enumerate(papers):
                    logger.info(f"  è®ºæ–‡ {i+1}: {paper.get('title', 'æœªçŸ¥æ ‡é¢˜')}")
                search_success = True
                break
            else:
                logger.warning(f"! æœç´¢ '{search_term}' æœªè¿”å›ç»“æœ")
        except Exception as e:
            logger.error(f"âœ— æœç´¢ '{search_term}' å¤±è´¥: {e}")
    
    if not search_success:
        logger.warning("! æ‰€æœ‰æœç´¢è¯éƒ½æœªè¿”å›ç»“æœï¼Œå¯èƒ½æ˜¯ç½‘ç«™ç»“æ„å˜åŒ–")
    
    logger.info("åŸºæœ¬åŠŸèƒ½æµ‹è¯•å®Œæˆï¼")
    return True


if __name__ == "__main__":
    success = test_basic_functionality()
    if success:
        logger.info("ğŸ‰ æ‰€æœ‰åŸºæœ¬åŠŸèƒ½æµ‹è¯•é€šè¿‡ï¼")
    else:
        logger.error("âŒ éƒ¨åˆ†åŠŸèƒ½æµ‹è¯•å¤±è´¥")
        sys.exit(1) 