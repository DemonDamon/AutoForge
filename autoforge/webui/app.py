import gradio as gr
import os
import sys
import time
from pathlib import Path
from dotenv import load_dotenv

# --- è·¯å¾„å’ŒçŽ¯å¢ƒè®¾ç½® ---
# è‡ªåŠ¨åŠ è½½é¡¹ç›®æ ¹ç›®å½•ä¸‹çš„.envæ–‡ä»¶
try:
    load_dotenv(dotenv_path=Path(__file__).parent.parent.parent / '.env')
except Exception as e:
    print(f"Warning: Could not load .env file. {e}")

# å°†é¡¹ç›®æ ¹ç›®å½•æ·»åŠ åˆ°Pythonè·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent.parent))

from autoforge import AutoForgeAgent
from autoforge.llm import BaiLianClient, DeepSeekClient, OpenAIClient
from autoforge.llm.base import BaseLLMClient

# --- å…¨å±€å˜é‡å’Œå¸¸é‡ ---
LLM_PROVIDERS = {
    "Bailian": BaiLianClient,
    "DeepSeek": DeepSeekClient,
    "OpenAI": OpenAIClient,
}

LLM_MODELS = {
    "Bailian": ["qwen-plus", "qwen-max", "qwen-long"],
    "DeepSeek": ["deepseek-chat", "deepseek-coder"],
    "OpenAI": ["gpt-4-turbo", "gpt-4o", "gpt-3.5-turbo"],
}

LATEX_DELIMITERS = [{"left": "$$", "right": "$$", "display": True}, {"left": "$", "right": "$", "display": False}]


# --- åŽç«¯æ ¸å¿ƒé€»è¾‘ ---

def get_llm_client(provider_name: str, api_key: str = None, model_name: str = None) -> BaseLLMClient:
    """æ ¹æ®æä¾›å•†åç§°ã€API Keyå’Œæ¨¡åž‹èŽ·å–LLMå®¢æˆ·ç«¯å®žä¾‹"""
    if not api_key:
        api_key = os.getenv(f"{provider_name.upper()}_API_KEY")
    
    if not api_key:
        raise ValueError(f"æœªæä¾›API Keyï¼Œä¸”æœªåœ¨çŽ¯å¢ƒå˜é‡ä¸­æ‰¾åˆ° {provider_name.upper()}_API_KEY")

    client_class = LLM_PROVIDERS.get(provider_name)
    if not client_class:
        raise ValueError(f"ä¸æ”¯æŒçš„LLMæä¾›å•†: {provider_name}")
    
    # å¦‚æžœæœªæä¾›æ¨¡åž‹ï¼Œä½¿ç”¨è¯¥æä¾›å•†çš„é»˜è®¤æ¨¡åž‹
    if not model_name:
        model_name = LLM_MODELS.get(provider_name, [None])[0]
        
    return client_class(api_key=api_key, model=model_name)

def test_llm_connection(provider_name: str, api_key: str, model_name: str):
    """æµ‹è¯•LLMè¿žæŽ¥"""
    try:
        client = get_llm_client(provider_name, api_key, model_name)
        if client.validate_connection():
            return "âœ… è¿žæŽ¥æˆåŠŸ"
        else:
            return "âŒ è¿žæŽ¥å¤±è´¥: æœªçŸ¥é”™è¯¯"
    except Exception as e:
        return f"âŒ è¿žæŽ¥å¤±è´¥: {e}"


def run_analysis_pipeline(
    llm_provider, 
    api_key, 
    llm_model,
    manual_description,
    # top_k,
    # skip_experiment
):
    """è¿è¡Œå®Œæ•´çš„åˆ†æžæµç¨‹ï¼Œå¹¶æµå¼æ›´æ–°UI"""
    if not manual_description:
        raise gr.Error("éœ€æ±‚æè¿°ä¸èƒ½ä¸ºç©º", "è¯·è¾“å…¥æ‚¨çš„éœ€æ±‚ã€‚")

    output_dir = Path("outputs") / f"webui_run_{int(time.time())}"
    
    # åˆå§‹åŒ–çŠ¶æ€å’Œè¾“å‡ºå†…å®¹
    status_updates = []
    req_md_content = "åˆ†æžä¸­ï¼Œè¯·ç¨å€™..."
    model_search_content = "ç­‰å¾…ä¸­..."

    def update_status_msg(msg):
        """ä»…æ›´æ–°çŠ¶æ€æ–‡æœ¬"""
        status_updates.append(msg)
        return (
            "\n".join(status_updates),
            gr.Markdown(value=req_md_content, latex_delimiters=LATEX_DELIMITERS),
            gr.Markdown(value=model_search_content, latex_delimiters=LATEX_DELIMITERS)
        )

    yield update_status_msg("1. åˆå§‹åŒ–LLMå®¢æˆ·ç«¯...")
    try:
        llm_client = get_llm_client(llm_provider, api_key, llm_model)
        if not llm_client.validate_connection():
            raise ValueError("LLMè¿žæŽ¥éªŒè¯å¤±è´¥")
        yield update_status_msg("   âœ… LLMå®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")
    except Exception as e:
        yield update_status_msg(f"   âŒ LLMå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}")
        raise gr.Error("LLMå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥", str(e))

    yield update_status_msg("\n2. åˆ›å»ºAutoForge Agent...")
    try:
        agent = AutoForgeAgent(
            llm_client=llm_client,
            output_dir=str(output_dir)
        )
        yield update_status_msg("   âœ… Agentåˆ›å»ºæˆåŠŸ")
    except Exception as e:
        yield update_status_msg(f"   âŒ Agentåˆ›å»ºå¤±è´¥: {e}")
        raise gr.Error("Agentåˆ›å»ºå¤±è´¥", str(e))
        
    yield update_status_msg("\n3. å¼€å§‹æ‰§è¡Œåˆ†æžæµç¨‹...")
    
    try:
        # ä½¿ç”¨forå¾ªçŽ¯å¤„ç†ç”Ÿæˆå™¨è¿”å›žçš„æ¯ä¸ªé˜¶æ®µç»“æžœ
        for result in agent.run_full_pipeline(
            manual_description=manual_description,
            skip_experiment_execution=True, # MVPé˜¶æ®µå›ºå®šä¸ºTrue
        ):
            stage = result.get("stage")
            stage_result = result.get("result", {})
            
            if stage == "requirement_analysis":
                status_updates.append("   - âœ… éœ€æ±‚åˆ†æžå®Œæˆ")
                output_file = stage_result.get("output_file", "")
                if output_file and Path(output_file).exists():
                    req_md_content = Path(output_file).read_text(encoding="utf-8")
                else:
                    req_md_content = f"### éœ€æ±‚åˆ†æžæŠ¥å‘Š\n\næ–‡ä»¶æœªæ‰¾åˆ°æˆ–ç”Ÿæˆå¤±è´¥ã€‚\n(è·¯å¾„: {output_file or 'N/A'})"
                model_search_content = "åˆ†æžä¸­ï¼Œè¯·ç¨å€™..." # æ›´æ–°ä¸‹ä¸€é˜¶æ®µçš„çŠ¶æ€
                yield (
                    "\n".join(status_updates),
                    gr.Markdown(value=req_md_content, latex_delimiters=LATEX_DELIMITERS),
                    gr.Markdown(value=model_search_content, latex_delimiters=LATEX_DELIMITERS)
                )

            elif stage == "model_search":
                status_updates.append("   - âœ… æ¨¡åž‹æœç´¢å®Œæˆ")
                output_file = stage_result.get("output_file", "")
                if output_file and Path(output_file).exists():
                    model_search_content = Path(output_file).read_text(encoding="utf-8")
                else:
                    model_search_content = f"### æ¨¡åž‹æœç´¢æŠ¥å‘Š\n\næ–‡ä»¶æœªæ‰¾åˆ°æˆ–ç”Ÿæˆå¤±è´¥ã€‚\n(è·¯å¾„: {output_file or 'N/A'})"
                yield (
                    "\n".join(status_updates),
                    gr.Markdown(value=req_md_content, latex_delimiters=LATEX_DELIMITERS),
                    gr.Markdown(value=model_search_content, latex_delimiters=LATEX_DELIMITERS)
                )

    except Exception as e:
        status_updates.append(f"\n   âŒ åˆ†æžæµç¨‹å‡ºé”™: {e}")
        yield (
            "\n".join(status_updates),
            gr.Markdown(value=req_md_content, latex_delimiters=LATEX_DELIMITERS),
            gr.Markdown(value=model_search_content, latex_delimiters=LATEX_DELIMITERS)
        )
        raise gr.Error("åˆ†æžæµç¨‹æ‰§è¡Œå¤±è´¥", str(e))

    status_updates.append(f"\nðŸŽ‰ å…¨éƒ¨å®Œæˆï¼ç»“æžœä¿å­˜åœ¨ç›®å½•: {output_dir.resolve()}")
    yield (
        "\n".join(status_updates),
        gr.Markdown(value=req_md_content, latex_delimiters=LATEX_DELIMITERS),
        gr.Markdown(value=model_search_content, latex_delimiters=LATEX_DELIMITERS)
    )

# --- Gradio UI æž„å»º ---

with gr.Blocks(theme=gr.themes.Soft(), title="AutoForge WebUI") as demo:
    gr.Markdown("# AutoForge æ™ºèƒ½ä½“-AIè§£å†³æ–¹æ¡ˆæŽ¢ç´¢å·¥å…·")
    
    with gr.Row():
        # å·¦ä¾§ï¼šé…ç½®ä¸Žè¾“å…¥
        with gr.Column(scale=1):
            gr.Markdown("## 1. é…ç½®LLM")
            with gr.Group():
                llm_provider = gr.Dropdown(
                    list(LLM_PROVIDERS.keys()), 
                    label="é€‰æ‹©LLMæä¾›å•†", 
                    value="Bailian"
                )
                llm_model = gr.Dropdown(
                    LLM_MODELS["Bailian"],
                    label="é€‰æ‹©æ¨¡åž‹",
                    value=LLM_MODELS["Bailian"][0]
                )
                api_key_input = gr.Textbox(
                    label="API Key", 
                    placeholder="ç•™ç©ºåˆ™ä»ŽçŽ¯å¢ƒå˜é‡åŠ è½½",
                    type="password"
                )
                test_conn_btn = gr.Button("æµ‹è¯•è¿žæŽ¥")
                test_conn_output = gr.Markdown()
            
            gr.Markdown("## 2. è¾“å…¥éœ€æ±‚")
            with gr.Tabs():
                with gr.TabItem("æ‰‹åŠ¨æè¿°"):
                    manual_description_input = gr.Textbox(
                        label="è¯¦ç»†éœ€æ±‚æè¿°",
                        placeholder="ä¾‹å¦‚ï¼šæˆ‘éœ€è¦ä¸€ä¸ªå›¾åƒè´¨é‡è¯„ä¼°æ¨¡åž‹ï¼Œç”¨äºŽè‡ªåŠ¨è¯„ä¼°å›¾ç‰‡çš„è´¨é‡å¥½å...",
                        lines=15
                    )
                with gr.TabItem("æ–‡æ¡£åˆ†æž (å³å°†æŽ¨å‡º)"):
                    gr.Markdown("åŠŸèƒ½æ­£åœ¨å¼€å‘ä¸­...")

            # gr.Markdown("## 3. é«˜çº§é€‰é¡¹")
            # with gr.Accordion("å±•å¼€é…ç½®", open=False):
            #     top_k_slider = gr.Slider(10, 100, value=30, step=1, label="æ¨¡åž‹æœç´¢æ•°é‡ (Top K)")
            #     skip_experiment_checkbox = gr.Checkbox(True, label="è·³è¿‡å®žéªŒæ‰§è¡Œ")

            gr.Markdown("## 3. å¼€å§‹åˆ†æž")
            start_button = gr.Button("ðŸš€ å¼€å§‹åˆ†æž", variant="primary")

        # å³ä¾§ï¼šçŠ¶æ€ä¸Žç»“æžœ
        with gr.Column(scale=2):
            gr.Markdown("## åˆ†æžçŠ¶æ€")
            status_text = gr.Textbox(
                label="å®žæ—¶çŠ¶æ€æ›´æ–°", 
                lines=8, 
                max_lines=8,
                interactive=False,
                autoscroll=True
            )
            
            gr.Markdown("## åˆ†æžæŠ¥å‘Š")
            with gr.Tabs():
                with gr.TabItem("éœ€æ±‚åˆ†æž"):
                    req_analysis_md = gr.Markdown("è¿™é‡Œå°†æ˜¾ç¤ºéœ€æ±‚åˆ†æžæŠ¥å‘Š...", latex_delimiters=LATEX_DELIMITERS)
                    copy_req_btn = gr.Button("ðŸ“‹ å¤åˆ¶æŠ¥å‘Šå†…å®¹")
                with gr.TabItem("æ¨¡åž‹æœç´¢"):
                    model_search_md = gr.Markdown("è¿™é‡Œå°†æ˜¾ç¤ºæ¨¡åž‹æœç´¢æŠ¥å‘Š...", latex_delimiters=LATEX_DELIMITERS)
                    copy_model_btn = gr.Button("ðŸ“‹ å¤åˆ¶æŠ¥å‘Šå†…å®¹")

    # --- äº‹ä»¶ç»‘å®š ---
    
    def update_model_choices(provider_name):
        models = LLM_MODELS.get(provider_name, [])
        return gr.Dropdown(choices=models, value=models[0] if models else None)

    llm_provider.change(
        fn=update_model_choices,
        inputs=llm_provider,
        outputs=llm_model
    )

    copy_req_btn.click(
        None,
        req_analysis_md,
        None,
        js="""
        (req_md) => {
            const temp_ta = document.createElement('textarea');
            temp_ta.value = req_md;
            document.body.appendChild(temp_ta);
            temp_ta.select();
            document.execCommand('copy');
            document.body.removeChild(temp_ta);
            
            // A simple way to give feedback.
            const original_text = this.querySelector('button').innerText;
            this.querySelector('button').innerText = 'âœ… å·²å¤åˆ¶!';
            setTimeout(() => { this.querySelector('button').innerText = original_text; }, 2000);
        }
        """
    )
    
    copy_model_btn.click(
        None,
        model_search_md,
        None,
        js="""
        (model_md) => {
            const temp_ta = document.createElement('textarea');
            temp_ta.value = model_md;
            document.body.appendChild(temp_ta);
            temp_ta.select();
            document.execCommand('copy');
            document.body.removeChild(temp_ta);
            
            const original_text = this.querySelector('button').innerText;
            this.querySelector('button').innerText = 'âœ… å·²å¤åˆ¶!';
            setTimeout(() => { this.querySelector('button').innerText = original_text; }, 2000);
        }
        """
    )

    test_conn_btn.click(
        fn=test_llm_connection,
        inputs=[llm_provider, api_key_input, llm_model],
        outputs=[test_conn_output]
    )
    
    start_button.click(
        fn=run_analysis_pipeline,
        inputs=[
            llm_provider,
            api_key_input,
            llm_model,
            manual_description_input,
            # top_k_slider,
            # skip_experiment_checkbox
        ],
        outputs=[
            status_text,
            req_analysis_md,
            model_search_md
        ]
    )

if __name__ == "__main__":
    demo.launch(share=True) 