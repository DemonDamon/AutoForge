"""
Papers with Code çˆ¬è™«æ¨¡å—
ç”¨äºçˆ¬å– Papers with Code ç½‘ç«™ä¸Šçš„è®ºæ–‡ä¿¡æ¯å’Œç›¸å…³ä»£ç ä»“åº“
"""

import os
import json
import time
import logging
import requests
from pathlib import Path
from typing import Dict, List, Optional, Any, Tuple
from urllib.parse import urljoin, urlparse, parse_qs, urlencode
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed
import random

from bs4 import BeautifulSoup
import re
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

logger = logging.getLogger(__name__)


class PapersWithCodeCrawler:
    """Papers with Code çˆ¬è™«"""
    
    def __init__(self, 
                 base_url: str = "https://paperswithcode.com",
                 output_dir: str = "outputs/pwc_papers",
                 max_workers: int = 4,
                 delay: float = 1.0,
                 max_retries: int = 3,
                 timeout: int = 60):
        """
        åˆå§‹åŒ–çˆ¬è™«
        
        Args:
            base_url: Papers with Code ç½‘ç«™åŸºç¡€URL
            output_dir: è¾“å‡ºç›®å½•
            max_workers: å¹¶å‘çˆ¬å–çº¿ç¨‹æ•°
            delay: è¯·æ±‚é—´éš”ï¼ˆç§’ï¼‰
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
            timeout: è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
        """
        self.base_url = base_url
        self.output_dir = Path(output_dir)
        self.max_workers = max_workers
        self.delay = delay
        self.max_retries = max_retries
        self.timeout = timeout
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # è®¾ç½®è¯·æ±‚å¤´
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Cache-Control': 'max-age=0',
            'Upgrade-Insecure-Requests': '1',
            'Referer': base_url,
        }
        
        # éšæœºç”¨æˆ·ä»£ç†
        self.user_agents = [
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.5 Safari/605.1.15',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/116.0',
            'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        ]
        
        # è®¾ç½®é‡è¯•ç­–ç•¥
        retry_strategy = Retry(
            total=self.max_retries,
            status_forcelist=[429, 500, 502, 503, 504],
            allowed_methods=["GET", "POST"],
            backoff_factor=1,
        )
        adapter = HTTPAdapter(max_retries=retry_strategy)
        
        # ä¼šè¯å¯¹è±¡
        self.session = requests.Session()
        self.session.mount("https://", adapter)
        self.session.mount("http://", adapter)
        self.session.headers.update(self.headers)
    
    def _get_with_retry(self, url, params=None, **kwargs):
        """å‘é€GETè¯·æ±‚ï¼Œå¸¦é‡è¯•æœºåˆ¶"""
        for attempt in range(self.max_retries + 1):
            try:
                # éšæœºä½¿ç”¨ä¸åŒçš„ç”¨æˆ·ä»£ç†
                self.session.headers.update({
                    'User-Agent': random.choice(self.user_agents)
                })
                
                # æ·»åŠ éšæœºå»¶è¿Ÿï¼Œé¿å…è¢«è¯†åˆ«ä¸ºçˆ¬è™«
                if attempt > 0:
                    jitter = random.uniform(0.5, 2.0)
                    sleep_time = self.delay * (2 ** attempt) * jitter
                    logger.info(f"ç¬¬ {attempt} æ¬¡é‡è¯•ï¼Œç­‰å¾… {sleep_time:.2f} ç§’...")
                    time.sleep(sleep_time)
                else:
                    time.sleep(self.delay)
                
                # å‘é€è¯·æ±‚
                response = self.session.get(
                    url, 
                    params=params, 
                    timeout=self.timeout,
                    **kwargs
                )
                response.raise_for_status()
                return response
            
            except (requests.exceptions.Timeout, 
                    requests.exceptions.ConnectionError) as e:
                logger.warning(f"è¯·æ±‚è¶…æ—¶æˆ–è¿æ¥é”™è¯¯ (å°è¯• {attempt+1}/{self.max_retries+1}): {e}")
                if attempt == self.max_retries:
                    raise
            
            except requests.exceptions.HTTPError as e:
                logger.warning(f"HTTPé”™è¯¯ (å°è¯• {attempt+1}/{self.max_retries+1}): {e}")
                if e.response.status_code == 404:
                    # 404é”™è¯¯ä¸é‡è¯•
                    raise
                if attempt == self.max_retries:
                    raise
            
            except Exception as e:
                logger.warning(f"æœªçŸ¥é”™è¯¯ (å°è¯• {attempt+1}/{self.max_retries+1}): {e}")
                if attempt == self.max_retries:
                    raise
    
    def crawl_trending_papers(self, top_k: int = 10) -> List[Dict[str, Any]]:
        """
        çˆ¬å–çƒ­é—¨è®ºæ–‡åˆ—è¡¨
        
        Args:
            top_k: çˆ¬å–å‰Kç¯‡è®ºæ–‡
            
        Returns:
            è®ºæ–‡ä¿¡æ¯åˆ—è¡¨
        """
        logger.info(f"å¼€å§‹çˆ¬å– Papers with Code çƒ­é—¨è®ºæ–‡...")
        
        url = f"{self.base_url}/"
        
        try:
            response = self._get_with_retry(url)
            
            # ç¡®ä¿æ­£ç¡®ç¼–ç 
            if response.encoding == 'ISO-8859-1':
                response.encoding = 'utf-8'
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # æŸ¥æ‰¾çƒ­é—¨è®ºæ–‡éƒ¨åˆ† - æ ¹æ®æœ€æ–°çš„é¡µé¢ç»“æ„è¿›è¡ŒæŸ¥æ‰¾
            papers = []
            
            # å°è¯•å¤šç§å¯èƒ½çš„å¡ç‰‡é€‰æ‹©å™¨
            paper_items = (
                soup.find_all('div', class_='row paper-card') or
                soup.find_all('div', class_='paper-card') or
                soup.select('.paper-card') or
                soup.select('.paper-card-container') or
                soup.select('.infinite-item')
            )
            
            logger.info(f"æ‰¾åˆ° {len(paper_items)} ä¸ªè®ºæ–‡å¡ç‰‡")
            
            # åªå¤„ç†å‰ top_k ä¸ªç»“æœ
            paper_items = paper_items[:top_k]
            
            for item in paper_items:
                paper_info = self._parse_paper_item(item)
                if paper_info:
                    papers.append(paper_info)
            
            # ä¿å­˜è®ºæ–‡åˆ—è¡¨
            self._save_paper_list("trending", papers)
            
            logger.info(f"æˆåŠŸçˆ¬å– {len(papers)} ç¯‡è®ºæ–‡")
            return papers
            
        except Exception as e:
            logger.error(f"çˆ¬å–çƒ­é—¨è®ºæ–‡å¤±è´¥: {e}")
            raise
    
    def crawl_papers_by_area(self, area: str, sort: str = "trending", top_k: int = 10) -> List[Dict[str, Any]]:
        """
        æ ¹æ®ç ”ç©¶é¢†åŸŸçˆ¬å–è®ºæ–‡
        
        Args:
            area: ç ”ç©¶é¢†åŸŸï¼Œå¦‚ 'computer-vision', 'natural-language-processing'
            sort: æ’åºæ–¹å¼ï¼Œ'trending' æˆ– 'newest'
            top_k: çˆ¬å–æ•°é‡
            
        Returns:
            è®ºæ–‡ä¿¡æ¯åˆ—è¡¨
        """
        logger.info(f"å¼€å§‹çˆ¬å–é¢†åŸŸ '{area}' çš„è®ºæ–‡...")
        
        # æ„å»ºURL
        url = f"{self.base_url}/area/{area}"
        params = None
        if sort == "newest":
            params = {"order": "newest"}
        
        try:
            response = self._get_with_retry(url, params=params)
            
            # ç¡®ä¿æ­£ç¡®ç¼–ç 
            if response.encoding == 'ISO-8859-1':
                response.encoding = 'utf-8'
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            papers = []
            
            # å°è¯•å¤šç§å¯èƒ½çš„å¡ç‰‡é€‰æ‹©å™¨
            paper_items = (
                soup.find_all('div', class_='row paper-card') or
                soup.find_all('div', class_='paper-card') or
                soup.select('.paper-card') or
                soup.select('.paper-card-container') or
                soup.select('.infinite-item')
            )
            
            logger.info(f"æ‰¾åˆ° {len(paper_items)} ä¸ªè®ºæ–‡å¡ç‰‡")
            
            # åªå¤„ç†å‰ top_k ä¸ªç»“æœ
            paper_items = paper_items[:top_k]
            
            for item in paper_items:
                paper_info = self._parse_paper_item(item)
                if paper_info:
                    paper_info['area'] = area
                    papers.append(paper_info)
            
            # ä¿å­˜è®ºæ–‡åˆ—è¡¨
            self._save_paper_list(f"{area}_{sort}", papers)
            
            logger.info(f"æˆåŠŸçˆ¬å– {len(papers)} ç¯‡è®ºæ–‡")
            return papers
            
        except Exception as e:
            logger.error(f"çˆ¬å–é¢†åŸŸè®ºæ–‡å¤±è´¥: {e}")
            raise
    
    def crawl_paper_details(self, paper_url: str) -> Dict[str, Any]:
        """
        çˆ¬å–å•ç¯‡è®ºæ–‡çš„è¯¦ç»†ä¿¡æ¯
        
        Args:
            paper_url: è®ºæ–‡é¡µé¢URL
            
        Returns:
            è®ºæ–‡è¯¦ç»†ä¿¡æ¯
        """
        logger.info(f"å¼€å§‹çˆ¬å–è®ºæ–‡è¯¦æƒ…: {paper_url}")
        
        try:
            if not paper_url.startswith('http'):
                paper_url = urljoin(self.base_url, paper_url)
            
            response = self._get_with_retry(paper_url)
            
            # ç¡®ä¿æ­£ç¡®ç¼–ç 
            if response.encoding == 'ISO-8859-1':
                response.encoding = 'utf-8'
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # æå–è®ºæ–‡ä¿¡æ¯
            paper_details = {
                'url': paper_url,
                'crawled_at': datetime.now().isoformat()
            }
            
            # æ ‡é¢˜
            title_elem = soup.find('h1')
            if title_elem:
                paper_details['title'] = title_elem.text.strip()
            
            # ä½œè€…
            authors_elem = soup.find('div', class_='authors')
            if authors_elem:
                paper_details['authors'] = [a.text.strip() for a in authors_elem.find_all('a')]
            
            # æ‘˜è¦
            abstract_elem = soup.find('div', class_='paper-abstract')
            if abstract_elem:
                paper_details['abstract'] = abstract_elem.text.strip()
            
            # æ ‡ç­¾
            tags = []
            tag_elems = soup.find_all('a', class_='badge badge-secondary')
            for tag in tag_elems:
                tags.append(tag.text.strip())
            paper_details['tags'] = tags
            
            # GitHubä»“åº“é“¾æ¥
            github_repos = self._extract_github_repos(soup)
            paper_details['github_repos'] = github_repos
            
            # ä»£ç å®ç°åˆ—è¡¨
            implementations = self._extract_implementations(soup)
            paper_details['implementations'] = implementations
            
            # è®ºæ–‡é“¾æ¥ï¼ˆarxivç­‰ï¼‰
            paper_links = self._extract_paper_links(soup)
            paper_details['paper_links'] = paper_links
            
            return paper_details
            
        except Exception as e:
            logger.error(f"çˆ¬å–è®ºæ–‡è¯¦æƒ…å¤±è´¥: {e}")
            raise
    
    def crawl_papers_batch(self, 
                          papers: List[Dict[str, Any]], 
                          fetch_details: bool = True) -> List[Dict[str, Any]]:
        """
        æ‰¹é‡çˆ¬å–è®ºæ–‡è¯¦æƒ…
        
        Args:
            papers: è®ºæ–‡åˆ—è¡¨ï¼ˆéœ€åŒ…å«urlå­—æ®µï¼‰
            fetch_details: æ˜¯å¦çˆ¬å–è¯¦ç»†ä¿¡æ¯
            
        Returns:
            åŒ…å«è¯¦ç»†ä¿¡æ¯çš„è®ºæ–‡åˆ—è¡¨
        """
        if not fetch_details:
            return papers
        
        logger.info(f"å¼€å§‹æ‰¹é‡çˆ¬å– {len(papers)} ç¯‡è®ºæ–‡çš„è¯¦ç»†ä¿¡æ¯...")
        
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            future_to_paper = {
                executor.submit(self.crawl_paper_details, paper['url']): paper
                for paper in papers if 'url' in paper
            }
            
            detailed_papers = []
            for future in as_completed(future_to_paper):
                paper = future_to_paper[future]
                try:
                    details = future.result()
                    # åˆå¹¶åŸºæœ¬ä¿¡æ¯å’Œè¯¦ç»†ä¿¡æ¯
                    paper.update(details)
                    detailed_papers.append(paper)
                except Exception as e:
                    logger.error(f"çˆ¬å–è®ºæ–‡è¯¦æƒ…å¤±è´¥: {e}")
                    detailed_papers.append(paper)
        
        return detailed_papers
    
    def search_papers(self, query: str, top_k: int = 10) -> List[Dict[str, Any]]:
        """
        æœç´¢è®ºæ–‡
        
        Args:
            query: æœç´¢å…³é”®è¯
            top_k: è¿”å›ç»“æœæ•°é‡
            
        Returns:
            è®ºæ–‡åˆ—è¡¨
        """
        logger.info(f"æœç´¢è®ºæ–‡: {query}")
        
        # æ„å»ºæœç´¢URL
        # ä¾‹å¦‚: https://paperswithcode.com/search?q_meta=&q_type=&q=2
        url = f"{self.base_url}/search"
        params = {'q': query, 'q_meta': '', 'q_type': ''}
        
        # è°ƒè¯•ä¿¡æ¯ï¼šæ‰“å°å®Œæ•´çš„è¯·æ±‚URL
        full_url = url + "?" + urlencode(params)
        logger.info(f"ğŸ” è¯·æ±‚URL: {full_url}")
        logger.info(f"ğŸ” è¯·æ±‚å‚æ•°: {params}")
        
        try:
            response = self._get_with_retry(url, params=params)
            
            # è°ƒè¯•ä¿¡æ¯ï¼šå“åº”çŠ¶æ€
            logger.info(f"ğŸ“¡ å“åº”çŠ¶æ€ç : {response.status_code}")
            logger.info(f"ğŸ“¡ å“åº”å†…å®¹é•¿åº¦: {len(response.text)} å­—ç¬¦")
            logger.info(f"ğŸ“¡ å“åº”ç¼–ç : {response.encoding}")
            logger.info(f"ğŸ“¡ å“åº”Content-Type: {response.headers.get('Content-Type', 'Unknown')}")
            
            # ç¡®ä¿æ­£ç¡®ç¼–ç 
            if response.encoding == 'ISO-8859-1':
                response.encoding = 'utf-8'
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # è°ƒè¯•ä¿¡æ¯ï¼šé¡µé¢ç»“æ„åˆ†æ
            logger.info(f"ğŸ” é¡µé¢æ ‡é¢˜: {soup.title.string if soup.title else 'No title'}")
            
            # ä¿å­˜å“åº”å†…å®¹åˆ°æ–‡ä»¶ä»¥ä¾¿è°ƒè¯•
            debug_file = self.output_dir / f"debug_search_{query.replace(' ', '_')}.html"
            with open(debug_file, 'w', encoding='utf-8') as f:
                f.write(response.text)
            logger.info(f"ğŸ” è°ƒè¯•é¡µé¢å·²ä¿å­˜åˆ°: {debug_file}")
            
            papers = []
            
            # å°è¯•å¤šç§å¯èƒ½çš„å¡ç‰‡é€‰æ‹©å™¨ï¼Œå¹¶è®°å½•æ¯ç§å°è¯•çš„ç»“æœ
            selectors = [
                ('div', {'class': 'row paper-card'}),
                ('div', {'class': 'paper-card'}),
                ('.paper-card', None),
                ('.paper-card-container', None),
                ('.infinite-item', None),
                ('.paper-list-item', None),
                ('.search-result', None),
                ('.item', None),
            ]
            
            paper_items = []
            for selector, attrs in selectors:
                if attrs:
                    items = soup.find_all(selector, attrs)
                else:
                    items = soup.select(selector)
                
                logger.info(f"ğŸ” é€‰æ‹©å™¨ '{selector}' {attrs or ''}: æ‰¾åˆ° {len(items)} ä¸ªå…ƒç´ ")
                
                if items:
                    paper_items = items
                    logger.info(f"âœ… ä½¿ç”¨é€‰æ‹©å™¨: {selector} {attrs or ''}")
                    break
            
            # å¦‚æœæ‰€æœ‰é€‰æ‹©å™¨éƒ½æ²¡æ‰¾åˆ°ï¼Œå°è¯•æ›´é€šç”¨çš„é€‰æ‹©å™¨
            if not paper_items:
                logger.warning("ğŸ” å°è¯•é€šç”¨é€‰æ‹©å™¨...")
                generic_selectors = ['div[class*="paper"]', 'div[class*="item"]', 'article', '.result']
                for sel in generic_selectors:
                    items = soup.select(sel)
                    logger.info(f"ğŸ” é€šç”¨é€‰æ‹©å™¨ '{sel}': æ‰¾åˆ° {len(items)} ä¸ªå…ƒç´ ")
                    if items:
                        paper_items = items[:10]  # é™åˆ¶æ•°é‡é¿å…è¯¯åŒ¹é…
                        logger.info(f"âœ… ä½¿ç”¨é€šç”¨é€‰æ‹©å™¨: {sel}")
                        break
            
            logger.info(f"æœç´¢ '{query}' æ‰¾åˆ° {len(paper_items)} ä¸ªè®ºæ–‡å¡ç‰‡")
            
            # è°ƒè¯•ä¿¡æ¯ï¼šåˆ†ææ‰¾åˆ°çš„å…ƒç´ 
            if paper_items:
                logger.info(f"ğŸ” ç¬¬ä¸€ä¸ªå…ƒç´ çš„ç±»å: {paper_items[0].get('class', [])}")
                logger.info(f"ğŸ” ç¬¬ä¸€ä¸ªå…ƒç´ çš„HTMLç‰‡æ®µ: {str(paper_items[0])[:200]}...")
            else:
                # å¦‚æœæ²¡æ‰¾åˆ°ä»»ä½•è®ºæ–‡å¡ç‰‡ï¼Œåˆ†æé¡µé¢å¯èƒ½çš„ç»“æ„
                logger.warning("ğŸ” æœªæ‰¾åˆ°è®ºæ–‡å¡ç‰‡ï¼Œåˆ†æé¡µé¢ç»“æ„...")
                
                # æŸ¥æ‰¾æ‰€æœ‰å¯èƒ½åŒ…å«è®ºæ–‡ä¿¡æ¯çš„div
                all_divs = soup.find_all('div')
                logger.info(f"ğŸ” é¡µé¢æ€»å…±æœ‰ {len(all_divs)} ä¸ªdivå…ƒç´ ")
                
                # æŸ¥æ‰¾åŒ…å«"paper"å…³é”®è¯çš„ç±»å
                paper_classes = set()
                for div in all_divs:
                    classes = div.get('class', [])
                    for cls in classes:
                        if 'paper' in cls.lower() or 'item' in cls.lower() or 'result' in cls.lower():
                            paper_classes.add(cls)
                
                if paper_classes:
                    logger.info(f"ğŸ” å‘ç°å¯èƒ½çš„è®ºæ–‡ç›¸å…³ç±»å: {list(paper_classes)}")
                else:
                    logger.warning("ğŸ” æœªå‘ç°æ˜æ˜¾çš„è®ºæ–‡ç›¸å…³ç±»å")
                
                # æŸ¥çœ‹æ˜¯å¦æœ‰æœç´¢ç»“æœæç¤º
                no_results_indicators = [
                    'no results', 'no papers', 'not found', '0 results', 'nothing found'
                ]
                page_text = soup.get_text().lower()
                for indicator in no_results_indicators:
                    if indicator in page_text:
                        logger.warning(f"ğŸ” é¡µé¢å¯èƒ½æ˜¾ç¤ºæ— ç»“æœ: å‘ç°æ–‡æœ¬ '{indicator}'")
                        break
            
            # åªå¤„ç†å‰ top_k ä¸ªç»“æœ
            paper_items = paper_items[:top_k]
            
            for i, item in enumerate(paper_items):
                logger.info(f"ğŸ” è§£æç¬¬ {i+1} ä¸ªè®ºæ–‡é¡¹...")
                paper_info = self._parse_paper_item(item)
                if paper_info:
                    papers.append(paper_info)
                    logger.info(f"âœ… æˆåŠŸè§£æè®ºæ–‡: {paper_info.get('title', 'Unknown')}")
                else:
                    logger.warning(f"âŒ è§£æç¬¬ {i+1} ä¸ªè®ºæ–‡é¡¹å¤±è´¥")
            
            # ä¿å­˜æœç´¢ç»“æœ
            self._save_paper_list(f"search_{query}", papers)
            
            logger.info(f"ğŸ‰ æœç´¢å®Œæˆï¼ŒæˆåŠŸè§£æ {len(papers)} ç¯‡è®ºæ–‡")
            return papers
            
        except Exception as e:
            logger.error(f"æœç´¢è®ºæ–‡å¤±è´¥: {e}")
            raise
    
    def _parse_paper_item(self, item) -> Optional[Dict[str, Any]]:
        """è§£æè®ºæ–‡åˆ—è¡¨é¡¹"""
        try:
            paper_info = {}
            
            # è®°å½•åŸå§‹HTMLä¾¿äºè°ƒè¯•
            # paper_info['_debug_html'] = str(item)
            
            # æå–æ ‡é¢˜å’Œé“¾æ¥ï¼ˆå°è¯•å¤šç§å¯èƒ½çš„é€‰æ‹©å™¨ï¼‰
            title_elem = (
                item.find('h1') or 
                item.find('h2') or 
                item.find('h3') or
                item.find('a', class_='paper-card-title') or
                item.select_one('.paper-card-title') or
                item.select_one('.item-title') or
                item.select_one('h1 a') or
                item.select_one('h2 a') or
                item.select_one('h3 a')
            )
            
            if title_elem:
                # ç›´æ¥ä»æ ‡é¢˜å…ƒç´ è·å–æ–‡æœ¬
                if hasattr(title_elem, 'text'):
                    paper_info['title'] = title_elem.text.strip()
                
                # è·å–é“¾æ¥
                if title_elem.name == 'a':
                    link_elem = title_elem
                else:
                    link_elem = title_elem.find('a')
                
                if link_elem and link_elem.has_attr('href'):
                    paper_info['url'] = urljoin(self.base_url, link_elem.get('href', ''))
            
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ ‡é¢˜æˆ–é“¾æ¥ï¼Œè¿”å›None
            if 'title' not in paper_info or 'url' not in paper_info:
                return None
            
            # ä½œè€…ï¼ˆå°è¯•å¤šç§å¯èƒ½çš„é€‰æ‹©å™¨ï¼‰
            authors_elem = (
                item.find('div', class_='authors') or
                item.find('div', class_='author-section') or
                item.select_one('.authors') or
                item.select_one('.author-section')
            )
            
            if authors_elem:
                paper_info['authors'] = authors_elem.text.strip()
            
            # æ˜Ÿæ ‡æ•°ï¼ˆå°è¯•å¤šç§å¯èƒ½çš„é€‰æ‹©å™¨ï¼‰
            stars_elem = (
                item.find('span', class_='stars-accumulated') or
                item.find('span', class_='badge badge-stars') or
                item.select_one('.stars-accumulated') or
                item.select_one('.stars')
            )
            
            if stars_elem:
                paper_info['stars'] = stars_elem.text.strip()
            
            # ä»»åŠ¡æ ‡ç­¾
            tasks = []
            tasks_elem = (
                item.find('div', class_='task') or
                item.find('div', class_='tasks') or
                item.select_one('.task') or
                item.select_one('.tasks')
            )
            
            if tasks_elem:
                task_links = tasks_elem.find_all('a')
                for task in task_links:
                    tasks.append(task.text.strip())
                
                if tasks:
                    paper_info['tasks'] = tasks
            
            # ä»£ç å®ç°æ•°é‡
            impl_count_pattern = re.compile(r'(\d+)\s+implementations?', re.I)
            
            # æ–¹æ³•1ï¼šä»æ–‡æœ¬ä¸­æŸ¥æ‰¾
            for span in item.find_all('span'):
                match = impl_count_pattern.search(span.text)
                if match:
                    paper_info['implementation_count'] = int(match.group(1))
                    break
            
            # æ–¹æ³•2ï¼šå¦‚æœæ²¡æ‰¾åˆ°ï¼Œå°è¯•å…¶ä»–å…ƒç´ 
            if 'implementation_count' not in paper_info:
                for div in item.find_all('div'):
                    match = impl_count_pattern.search(div.text)
                    if match:
                        paper_info['implementation_count'] = int(match.group(1))
                        break
            
            # æ·»åŠ æ—¥æœŸï¼ˆå¦‚æœæœ‰ï¼‰
            date_elem = (
                item.find('div', class_='date') or
                item.find('span', class_='date') or
                item.select_one('.date')
            )
            
            if date_elem:
                paper_info['date'] = date_elem.text.strip()
            
            return paper_info if paper_info else None
            
        except Exception as e:
            logger.error(f"è§£æè®ºæ–‡é¡¹å¤±è´¥: {e}")
            return None
    
    def _extract_github_repos(self, soup) -> List[Dict[str, str]]:
        """æå–GitHubä»“åº“é“¾æ¥"""
        repos = []
        
        # æŸ¥æ‰¾æ‰€æœ‰GitHubé“¾æ¥
        github_links = soup.find_all('a', href=re.compile(r'github\.com/[\w-]+/[\w-]+'))
        
        for link in github_links:
            repo_url = link.get('href', '')
            if repo_url:
                # æå–ä»“åº“ä¿¡æ¯
                match = re.search(r'github\.com/([\w-]+)/([\w-]+)', repo_url)
                if match:
                    owner, repo_name = match.groups()
                    
                    repo_info = {
                        'url': repo_url,
                        'owner': owner,
                        'repo_name': repo_name,
                        'full_name': f"{owner}/{repo_name}"
                    }
                    
                    # è·å–æ˜Ÿæ ‡æ•°ç­‰ä¿¡æ¯ï¼ˆå¦‚æœé¡µé¢ä¸Šæœ‰ï¼‰
                    parent = link.parent
                    if parent:
                        stars_elem = parent.find('span', class_='stars')
                        if stars_elem:
                            repo_info['stars'] = stars_elem.text.strip()
                    
                    repos.append(repo_info)
        
        # å»é‡
        seen = set()
        unique_repos = []
        for repo in repos:
            if repo['full_name'] not in seen:
                seen.add(repo['full_name'])
                unique_repos.append(repo)
        
        return unique_repos
    
    def _extract_implementations(self, soup) -> List[Dict[str, Any]]:
        """æå–ä»£ç å®ç°åˆ—è¡¨"""
        implementations = []
        
        # æŸ¥æ‰¾å®ç°éƒ¨åˆ†ï¼ˆå°è¯•å¤šç§å¯èƒ½çš„é€‰æ‹©å™¨ï¼‰
        impl_section = (
            soup.find('div', id='implementations') or 
            soup.find('div', class_='implementations') or
            soup.select_one('#implementations') or
            soup.select_one('.implementations')
        )
        
        if not impl_section:
            return implementations
        
        # å°è¯•å¤šç§å¯èƒ½çš„å®ç°é¡¹é€‰æ‹©å™¨
        impl_items = (
            impl_section.find_all('div', class_='row') or
            impl_section.find_all('div', class_='implementation-card') or
            impl_section.select('.implementation-card') or
            impl_section.select('.row')
        )
        
        for item in impl_items:
            impl_info = {}
            
            # æ¡†æ¶ä¿¡æ¯
            framework_elem = (
                item.find('span', class_='framework-badge') or
                item.select_one('.framework-badge') or
                item.select_one('.framework')
            )
            
            if framework_elem:
                impl_info['framework'] = framework_elem.text.strip()
            
            # GitHubé“¾æ¥
            github_link = item.find('a', href=re.compile(r'github\.com'))
            if github_link:
                impl_info['github_url'] = github_link.get('href', '')
                impl_info['title'] = github_link.text.strip()
            
            # æ˜Ÿæ ‡æ•°
            stars_elem = (
                item.find('span', class_='stars') or
                item.select_one('.stars')
            )
            
            if stars_elem:
                impl_info['stars'] = stars_elem.text.strip()
            
            if impl_info:
                implementations.append(impl_info)
        
        return implementations
    
    def _extract_paper_links(self, soup) -> Dict[str, str]:
        """æå–è®ºæ–‡é“¾æ¥ï¼ˆarxivç­‰ï¼‰"""
        links = {}
        
        # ArXivé“¾æ¥
        arxiv_link = soup.find('a', href=re.compile(r'arxiv\.org'))
        if arxiv_link:
            links['arxiv'] = arxiv_link.get('href', '')
        
        # PDFé“¾æ¥
        pdf_link = soup.find('a', text=re.compile(r'PDF', re.I))
        if pdf_link:
            links['pdf'] = pdf_link.get('href', '')
        
        # å…¶ä»–è®ºæ–‡é“¾æ¥ï¼ˆå°è¯•å¤šç§å¯èƒ½çš„é€‰æ‹©å™¨ï¼‰
        paper_section = (
            soup.find('div', class_='paper-links') or
            soup.find('div', class_='paper-resources') or
            soup.select_one('.paper-links') or
            soup.select_one('.paper-resources')
        )
        
        if paper_section:
            for link in paper_section.find_all('a'):
                text = link.text.strip().lower()
                href = link.get('href', '')
                if href and text:
                    links[text] = href
        
        return links
    
    def _save_paper_list(self, category: str, papers: List[Dict[str, Any]]):
        """ä¿å­˜è®ºæ–‡åˆ—è¡¨"""
        # åˆ›å»ºåˆ†ç±»ç›®å½•
        category_dir = self.output_dir / category
        category_dir.mkdir(parents=True, exist_ok=True)
        
        # ä¿å­˜ä¸ºJSON
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"papers_{timestamp}.json"
        filepath = category_dir / filename
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump({
                'category': category,
                'count': len(papers),
                'crawled_at': datetime.now().isoformat(),
                'papers': papers
            }, f, ensure_ascii=False, indent=2)
        
        logger.info(f"è®ºæ–‡åˆ—è¡¨å·²ä¿å­˜åˆ°: {filepath}")
    
    def get_research_areas(self) -> List[str]:
        """è·å–æ‰€æœ‰ç ”ç©¶é¢†åŸŸ"""
        return [
            'computer-vision',
            'natural-language-processing',
            'reinforcement-learning',
            'machine-learning',
            'audio',
            'robotics',
            'graphs',
            'time-series',
            'adversarial',
            'knowledge-base',
            'medical',
            'speech',
            'reasoning',
            'music',
            'playing-games'
        ] 